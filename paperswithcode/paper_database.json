[
    {
        "title": "ImageBind: One Embedding Space To Bind Them All",
        "authors": "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra",
        "gitlab": "facebookresearch/imagebind",
        "date": "9 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/paper/2305.05665.jpg",
        "abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",
        "strip_abstract": "We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.",
        "arxiv_url": "https://arxiv.org/pdf/2305.05665v1.pdf",
        "entity_stars": "4,850",
        "stars_accumulated": "13.53 stars / hour",
        "paper_task": [
            "Cross-Modal Retrieval",
            "Retrieval",
            "Zero-Shot Learning"
        ],
        "code": [
            "https://github.com/facebookresearch/imagebind"
        ]
    },
    {
        "title": "Shap-E: Generating Conditional 3D Implicit Functions",
        "authors": "Heewoo Jun, Alex Nichol",
        "gitlab": "openai/shap-e",
        "date": "3 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/paper/2305.02463.jpg",
        "abstract": "We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.",
        "strip_abstract": "We present Shap-E, a conditional generative model for 3D assets.",
        "arxiv_url": "https://arxiv.org/pdf/2305.02463v1.pdf",
        "entity_stars": "7,833",
        "stars_accumulated": "3.56 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/openai/shap-e"
        ]
    },
    {
        "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
        "authors": "Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, Ting Liu",
        "gitlab": "scir-hi/huatuo-llama-med-chinese",
        "date": "14 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/paper/2304.06975.jpg",
        "abstract": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo, a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.",
        "strip_abstract": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks.",
        "arxiv_url": "https://arxiv.org/pdf/2304.06975v1.pdf",
        "entity_stars": "2,181",
        "stars_accumulated": "3.11 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/scir-hi/huatuo-llama-med-chinese"
        ]
    },
    {
        "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans",
        "authors": "Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen",
        "gitlab": "open-mmlab/multimodal-gpt",
        "date": "8 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/f6b5fe05-3198-4f0d-af3a-92f58761c9fa.jpg",
        "abstract": "We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint training of language-only and visual-language instructions with the \\emph{same} instruction template effectively improves dialogue performance. Various demos show the ability of continuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are at https://github.com/open-mmlab/Multimodal-GPT",
        "strip_abstract": "To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly.",
        "arxiv_url": "https://arxiv.org/pdf/2305.04790v2.pdf",
        "entity_stars": "804",
        "stars_accumulated": "2.43 stars / hour",
        "paper_task": [
            "Instruction Following",
            "Language Modelling"
        ],
        "code": [
            "https://github.com/open-mmlab/multimodal-gpt"
        ]
    },
    {
        "title": "InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language",
        "authors": "Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang, LiMin Wang, Ping Luo, Jifeng Dai, Yu Qiao",
        "gitlab": "opengvlab/internchat",
        "date": "9 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/92fc3441-b428-43e4-a0eb-bd38935d6cb8.jpg",
        "abstract": "We present an interactive visual framework named InternGPT, or iGPT for short. The framework integrates chatbots that have planning and reasoning capabilities, such as ChatGPT, with non-verbal instructions like pointing movements that enable users to directly manipulate images or videos on the screen. Pointing (including gestures, cursors, etc.) movements can provide more flexibility and precision in performing vision-centric tasks that require fine-grained control, editing, and generation of visual content. The name InternGPT stands for \\textbf{inter}action, \\textbf{n}onverbal, and \\textbf{chat}bots. Different from existing interactive systems that rely on pure language, by incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2. Additionally, in iGPT, an auxiliary control mechanism is used to improve the control capability of LLM, and a large vision-language model termed Husky is fine-tuned for high-quality multi-modal dialogue (impressing ChatGPT-3.5-turbo with 93.89\\% GPT-4 Quality). We hope this work can spark new ideas and directions for future interactive visual systems. Welcome to watch the code at https://github.com/OpenGVLab/InternGPT.",
        "strip_abstract": "Different from existing interactive systems that rely on pure language, by incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.",
        "arxiv_url": "https://arxiv.org/pdf/2305.05662v3.pdf",
        "entity_stars": "307",
        "stars_accumulated": "2.24 stars / hour",
        "paper_task": [
            "Language Modelling"
        ],
        "code": [
            "https://github.com/opengvlab/interngpt",
            "https://github.com/opengvlab/internchat"
        ]
    },
    {
        "title": "WebCPM: Interactive Web Search for Chinese Long-form Question Answering",
        "authors": "Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie zhou",
        "gitlab": "thunlp/webcpm",
        "date": "11 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/d7eeb417-fbd4-4a64-8667-08070af3d37f.jpg",
        "abstract": "Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 14,315 supporting facts and 121,330 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader, respectively.",
        "strip_abstract": "We recruit annotators to search for relevant information using our interface and then answer questions.",
        "arxiv_url": "https://arxiv.org/pdf/2305.06849v1.pdf",
        "entity_stars": "157",
        "stars_accumulated": "2.08 stars / hour",
        "paper_task": [
            "Information Retrieval",
            "Long Form Question Answering",
            "Question Answering",
            "Retrieval"
        ],
        "code": [
            "https://github.com/thunlp/webcpm"
        ]
    },
    {
        "title": "Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures",
        "authors": "Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or",
        "gitlab": "threestudio-project/threestudio",
        "date": "14 Nov 2022",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/e3ab7c3f-f1f0-40bc-9fd5-b861e6221d3b.jpg",
        "abstract": "Text-guided image generation has progressed rapidly in recent years, inspiring major breakthroughs in text-guided shape generation. Recently, it has been shown that using score distillation, one can successfully text-guide a NeRF model to generate a 3D object. We adapt the score distillation to the publicly available, and computationally efficient, Latent Diffusion Models, which apply the entire diffusion process in a compact latent space of a pretrained autoencoder. As NeRFs operate in image space, a naive solution for guiding them with latent score distillation would require encoding to the latent space at each guidance step. Instead, we propose to bring the NeRF to the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we show that while Text-to-3D models can generate impressive results, they are inherently unconstrained and may lack the ability to guide or enforce a specific 3D structure. To assist and direct the 3D generation, we propose to guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines the coarse structure of the desired object. Then, we present means to integrate such a constraint directly into a Latent-NeRF. This unique combination of text and shape guidance allows for increased control over the generation process. We also show that latent score distillation can be successfully applied directly on 3D meshes. This allows for generating high-quality textures on a given geometry. Our experiments validate the power of our different forms of guidance and the efficiency of using latent rendering. Implementation is available at https://github.com/eladrich/latent-nerf",
        "strip_abstract": "This unique combination of text and shape guidance allows for increased control over the generation process.",
        "arxiv_url": "https://arxiv.org/pdf/2211.07600v1.pdf",
        "entity_stars": "560",
        "stars_accumulated": "1.78 stars / hour",
        "paper_task": [
            "Image Generation",
            "Text to 3D"
        ],
        "code": [
            "https://github.com/eladrich/latent-nerf",
            "https://github.com/threestudio-project/threestudio"
        ]
    },
    {
        "title": "Personalize Segment Anything Model with One Shot",
        "authors": "Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li",
        "gitlab": "zrrskywalker/personalize-sam",
        "date": "4 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/2e309c41-62c7-4915-9e2e-8597ad341b7c.jpg",
        "abstract": "Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for personalized evaluation, and test our methods on video object segmentation with competitive performance. Besides, our approach can also enhance DreamBooth to personalize Stable Diffusion for text-to-image generation, which discards the background disturbance for better target appearance learning. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM",
        "strip_abstract": "Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models.",
        "arxiv_url": "https://arxiv.org/pdf/2305.03048v1.pdf",
        "entity_stars": "709",
        "stars_accumulated": "1.49 stars / hour",
        "paper_task": [
            "Image Generation",
            "Semantic Segmentation",
            "Text-to-Image Generation",
            "Video Object Segmentation",
            "Video Semantic Segmentation"
        ],
        "code": [
            "https://github.com/zrrskywalker/personalize-sam"
        ]
    },
    {
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
        "authors": "Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu",
        "gitlab": "mooler0410/llmspracticalguide",
        "date": "26 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/16d0f450-e5e7-4471-9a8b-42727da19551.gif",
        "abstract": "This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \\url{https://github.com/Mooler0410/LLMsPracticalGuide}.",
        "strip_abstract": "This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.",
        "arxiv_url": "https://arxiv.org/pdf/2304.13712v2.pdf",
        "entity_stars": "3,631",
        "stars_accumulated": "1.11 stars / hour",
        "paper_task": [
            "Language Modelling",
            "Natural Language Understanding",
            "Text Generation"
        ],
        "code": [
            "https://github.com/mooler0410/llmspracticalguide"
        ]
    },
    {
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
        "authors": "Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan",
        "gitlab": "IBM/Dromedary",
        "date": "4 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/79dce5b1-b99c-42b8-9d74-1ff8948d6356.jpg",
        "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
        "strip_abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable.",
        "arxiv_url": "https://arxiv.org/pdf/2305.03047v1.pdf",
        "entity_stars": "688",
        "stars_accumulated": "1.03 stars / hour",
        "paper_task": [
            "Language Modelling"
        ],
        "code": [
            "https://github.com/IBM/Dromedary"
        ]
    },
    {
        "title": "iDisc: Internal Discretization for Monocular Depth Estimation",
        "authors": "Luigi Piccinelli, Christos Sakaridis, Fisher Yu",
        "gitlab": "SysCV/idisc",
        "date": "13 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/7998a93b-c6ce-4387-b1a0-e63962fe327b.jpg",
        "abstract": "Monocular depth estimation is fundamental for 3D scene understanding and downstream applications. However, even under the supervised setup, it is still challenging and ill-posed due to the lack of full geometric constraints. Although a scene can consist of millions of pixels, there are fewer high-level patterns. We propose iDisc to learn those patterns with internal discretized representations. The method implicitly partitions the scene into a set of high-level patterns. In particular, our new module, Internal Discretization (ID), implements a continuous-discrete-continuous bottleneck to learn those concepts without supervision. In contrast to state-of-the-art methods, the proposed model does not enforce any explicit constraints or priors on the depth output. The whole network with the ID module can be trained end-to-end, thanks to the bottleneck module based on attention. Our method sets the new state of the art with significant improvements on NYU-Depth v2 and KITTI, outperforming all published methods on the official KITTI benchmark. iDisc can also achieve state-of-the-art results on surface normal estimation. Further, we explore the model generalization capability via zero-shot testing. We observe the compelling need to promote diversification in the outdoor scenario. Hence, we introduce splits of two autonomous driving datasets, DDAD and Argoverse. Code is available at http://vis.xyz/pub/idisc .",
        "strip_abstract": "Our method sets the new state of the art with significant improvements on NYU-Depth v2 and KITTI, outperforming all published methods on the official KITTI benchmark.",
        "arxiv_url": "https://arxiv.org/pdf/2304.06334v1.pdf",
        "entity_stars": "106",
        "stars_accumulated": "1.02 stars / hour",
        "paper_task": [
            "Autonomous Driving",
            "Depth Estimation",
            "Monocular Depth Estimation",
            "Scene Understanding",
            "Surface Normal Estimation",
            "Surface Normals Estimation"
        ],
        "code": [
            "https://github.com/SysCV/idisc"
        ]
    },
    {
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
        "authors": "Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim",
        "gitlab": "agi-edgerunners/plan-and-solve-prompting",
        "date": "6 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/17426cda-2afa-406d-b3d9-0f5e4e02d8c4.jpg",
        "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with \"Let's think step by step\" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
        "strip_abstract": "To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting.",
        "arxiv_url": "https://arxiv.org/pdf/2305.04091v1.pdf",
        "entity_stars": "99",
        "stars_accumulated": "1.01 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/agi-edgerunners/plan-and-solve-prompting"
        ]
    },
    {
        "title": "U$^2$-Net: Going Deeper with Nested U-Structure for Salient Object Detection",
        "authors": "Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R. Zaiane, Martin Jagersand",
        "gitlab": "nadermx/backgroundremover",
        "date": "18 May 2020",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/pgr-0001807202-aa87ced9.gif",
        "abstract": "In this paper, we design a simple yet powerful deep network architecture, U$^2$-Net, for salient object detection (SOD). The architecture of our U$^2$-Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U$^2$-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and U$^2$-Net$^{\\dagger}$ (4.7 MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available: https://github.com/NathanUA/U-2-Net.",
        "strip_abstract": "In this paper, we design a simple yet powerful deep network architecture, U$^2$-Net, for salient object detection (SOD).",
        "arxiv_url": "https://arxiv.org/pdf/2005.09007v3.pdf",
        "entity_stars": "4,182",
        "stars_accumulated": "0.96 stars / hour",
        "paper_task": [
            "Dichotomous Image Segmentation",
            "Image Classification",
            "Object Detection",
            "RGB Salient Object Detection",
            "Saliency Detection",
            "Salient Object Detection"
        ],
        "code": [
            "https://github.com/NathanUA/U-2-Net",
            "https://github.com/PaddlePaddle/PaddleSeg",
            "https://github.com/xuebinqin/U-2-Net",
            "https://github.com/lucidrains/imagen-pytorch",
            "https://github.com/nadermx/backgroundremover"
        ]
    },
    {
        "title": "Active Retrieval Augmented Generation",
        "authors": "Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig",
        "gitlab": "jzbjyb/flare",
        "date": "11 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/paper/2305.06983.jpg",
        "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic retrieval-augmented generation method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "strip_abstract": "We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic retrieval-augmented generation method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.",
        "arxiv_url": "https://arxiv.org/pdf/2305.06983v1.pdf",
        "entity_stars": "65",
        "stars_accumulated": "0.82 stars / hour",
        "paper_task": [
            "Retrieval"
        ],
        "code": [
            "https://github.com/jzbjyb/flare"
        ]
    },
    {
        "title": "Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation",
        "authors": "Junde Wu, Yu Zhang, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei Wang, Yanwu Xu, Yueming Jin",
        "gitlab": "wujunde/medical-sam-adapter",
        "date": "25 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/4d9e41fa-cac6-4099-a25c-2c791cbc96de.jpg",
        "abstract": "The Segment Anything Model (SAM) has recently gained popularity in the field of image segmentation. Thanks to its impressive capabilities in all-round segmentation tasks and its prompt-based interface, SAM has sparked intensive discussion within the community. It is even said by many prestigious experts that image segmentation task has been \"finished\" by SAM. However, medical image segmentation, although an important branch of the image segmentation family, seems not to be included in the scope of Segmenting \"Anything\". Many individual experiments and recent studies have shown that SAM performs subpar in medical image segmentation. A natural question is how to find the missing piece of the puzzle to extend the strong segmentation capability of SAM to medical image segmentation. In this paper, instead of fine-tuning the SAM model, we propose Med SAM Adapter, which integrates the medical specific domain knowledge to the segmentation model, by a simple yet effective adaptation technique. Although this work is still one of a few to transfer the popular NLP technique Adapter to computer vision cases, this simple implementation shows surprisingly good performance on medical image segmentation. A medical image adapted SAM, which we have dubbed Medical SAM Adapter (MSA), shows superior performance on 19 medical image segmentation tasks with various image modalities including CT, MRI, ultrasound image, fundus image, and dermoscopic images. MSA outperforms a wide range of state-of-the-art (SOTA) medical image segmentation methods, such as nnUNet, TransUNet, UNetr, MedSegDiff, and also outperforms the fully fine-turned MedSAM with a considerable performance gap. Code will be released at: https://github.com/WuJunde/Medical-SAM-Adapter.",
        "strip_abstract": "A medical image adapted SAM, which we have dubbed Medical SAM Adapter (MSA), shows superior performance on 19 medical image segmentation tasks with various image modalities including CT, MRI, ultrasound image, fundus image, and dermoscopic images.",
        "arxiv_url": "https://arxiv.org/pdf/2304.12620v5.pdf",
        "entity_stars": "139",
        "stars_accumulated": "0.81 stars / hour",
        "paper_task": [
            "Image Segmentation",
            "Medical Image Segmentation",
            "Semantic Segmentation"
        ],
        "code": [
            "https://github.com/wujunde/medical-sam-adapter"
        ]
    },
    {
        "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
        "authors": "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu",
        "gitlab": "luodian/otter",
        "date": "5 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/352a304b-a14d-4477-9f01-6ae32741bdf1.jpg",
        "abstract": "Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines.",
        "strip_abstract": "Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks.",
        "arxiv_url": "https://arxiv.org/pdf/2305.03726v1.pdf",
        "entity_stars": "601",
        "stars_accumulated": "0.80 stars / hour",
        "paper_task": [
            "Instruction Following"
        ],
        "code": [
            "https://github.com/luodian/otter"
        ]
    },
    {
        "title": "An Inverse Scaling Law for CLIP Training",
        "authors": "Xianhang Li, Zeyu Wang, Cihang Xie",
        "gitlab": "ucsc-vlaa/clipa",
        "date": "11 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/122e7d62-50bb-4f49-b0ab-ba50d3187712.jpg",
        "abstract": "CLIP, the first foundation model that connects images and text, has enabled many recent breakthroughs in computer vision. However, its associated training cost is prohibitively high, imposing a significant barrier to its widespread exploration. In this paper, we present a surprising finding that there exists an inverse scaling law for CLIP training, whereby the larger the image/text encoders used, the shorter the sequence length of image/text tokens that can be applied in training. Moreover, we showcase that the strategy for reducing image/text token length plays a crucial role in determining the quality of this scaling law. As a result of this finding, we are able to successfully train CLIP even by using academic resources. For example, on an A100 eight-GPU server, our CLIP models achieve zero-shot top-1 ImageNet accuracies of 63.2% in ~2 days, 67.8% in ~3 days, and 69.3% in ~4 days. By reducing the computation barrier associated with CLIP, we hope to inspire more research in this field, particularly from academics. Our code is available at https://github.com/UCSC-VLAA/CLIPA.",
        "strip_abstract": "CLIP, the first foundation model that connects images and text, has enabled many recent breakthroughs in computer vision.",
        "arxiv_url": "https://arxiv.org/pdf/2305.07017v1.pdf",
        "entity_stars": "47",
        "stars_accumulated": "0.78 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/ucsc-vlaa/clipa"
        ]
    },
    {
        "title": "PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces",
        "authors": "Yiqun Wang, Ivan Skorokhodov, Peter Wonka",
        "gitlab": "yiqun-wang/pet-neus",
        "date": "9 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/190d460c-5782-4f15-b859-b29eb5fb8514.jpg",
        "abstract": "A signed distance function (SDF) parametrized by an MLP is a common ingredient of neural surface reconstruction. We build on the successful recent method NeuS to extend it by three new components. The first component is to borrow the tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only. Using tri-planes leads to a more expressive data structure but will also introduce noise in the reconstructed surface. The second component is to use a new type of positional encoding with learnable weights to combat noise in the reconstruction process. We divide the features in the tri-plane into multiple frequency scales and modulate them with sin and cos functions of different frequencies. The third component is to use learnable convolution operations on the tri-plane features using self-attention convolution to produce features with different frequency bands. The experiments show that PET-NeuS achieves high-fidelity surface reconstruction on standard datasets. Following previous work and using the Chamfer metric as the most important way to measure surface reconstruction quality, we are able to improve upon the NeuS baseline by 57% on Nerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to 0.84). The qualitative evaluation reveals how our method can better control the interference of high-frequency noise. Code available at \\url{https://github.com/yiqun-wang/PET-NeuS}.",
        "strip_abstract": "The first component is to borrow the tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only.",
        "arxiv_url": "https://arxiv.org/pdf/2305.05594v1.pdf",
        "entity_stars": "79",
        "stars_accumulated": "0.78 stars / hour",
        "paper_task": [
            "Surface Reconstruction"
        ],
        "code": [
            "https://github.com/yiqun-wang/pet-neus"
        ]
    },
    {
        "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
        "authors": "Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser",
        "gitlab": "jimmyyhwu/tidybot",
        "date": "9 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/paper/2305.05658.jpg",
        "abstract": "For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.",
        "strip_abstract": "For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios.",
        "arxiv_url": "https://arxiv.org/pdf/2305.05658v1.pdf",
        "entity_stars": "125",
        "stars_accumulated": "0.76 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/jimmyyhwu/tidybot"
        ]
    },
    {
        "title": "PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model",
        "authors": "Juncai Peng, Yi Liu, Shiyu Tang, Yuying Hao, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Zhiliang Yu, Yuning Du, Qingqing Dang, Baohua Lai, Qiwen Liu, Xiaoguang Hu, dianhai yu, Yanjun Ma",
        "gitlab": "Deci-AI/super-gradients",
        "date": "6 Apr 2022",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/1f3bf3f5-d4b9-4f0e-9a0d-cdd71cb37197.jpg",
        "abstract": "Real-world applications have high demands for semantic segmentation methods. Although semantic segmentation has made remarkable leap-forwards with deep learning, the performance of real-time methods is not satisfactory. In this work, we propose PP-LiteSeg, a novel lightweight model for the real-time semantic segmentation task. Specifically, we present a Flexible and Lightweight Decoder (FLD) to reduce computation overhead of previous decoder. To strengthen feature representations, we propose a Unified Attention Fusion Module (UAFM), which takes advantage of spatial and channel attention to produce a weight and then fuses the input features with the weight. Moreover, a Simple Pyramid Pooling Module (SPPM) is proposed to aggregate global context with low computation cost. Extensive evaluations demonstrate that PP-LiteSeg achieves a superior trade-off between accuracy and speed compared to other methods. On the Cityscapes test set, PP-LiteSeg achieves 72.0% mIoU/273.6 FPS and 77.5% mIoU/102.6 FPS on NVIDIA GTX 1080Ti. Source code and models are available at PaddleSeg: https://github.com/PaddlePaddle/PaddleSeg.",
        "strip_abstract": "Real-world applications have high demands for semantic segmentation methods.",
        "arxiv_url": "https://arxiv.org/pdf/2204.02681v1.pdf",
        "entity_stars": "2,207",
        "stars_accumulated": "0.75 stars / hour",
        "paper_task": [
            "Real-Time Semantic Segmentation",
            "Semantic Segmentation"
        ],
        "code": [
            "https://github.com/PaddlePaddle/PaddleSeg",
            "https://github.com/Deci-AI/super-gradients"
        ]
    },
    {
        "title": "VideoChat: Chat-Centric Video Understanding",
        "authors": "Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, LiMin Wang, Yu Qiao",
        "gitlab": "opengvlab/ask-anything",
        "date": "10 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/ba5103d5-422a-41c2-a470-d85685fe702f.jpg",
        "abstract": "In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything",
        "strip_abstract": "In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system.",
        "arxiv_url": "https://arxiv.org/pdf/2305.06355v1.pdf",
        "entity_stars": "1,512",
        "stars_accumulated": "0.73 stars / hour",
        "paper_task": [
            "Video Understanding"
        ],
        "code": [
            "https://github.com/opengvlab/ask-anything"
        ]
    },
    {
        "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca",
        "authors": "Yiming Cui, Ziqing Yang, Xin Yao",
        "gitlab": "ymcui/chinese-llama-alpaca",
        "date": "17 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/3d8edefd-71e4-49a3-86cd-10f0ec010798.jpg",
        "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI). However, the expensive training and deployment of LLMs present challenges to transparent and open academic research. To address these issues, this project open-sources the Chinese LLaMA and Alpaca large models, emphasizing instruction fine-tuning. We expand the original LLaMA's Chinese vocabulary by adding 20K Chinese tokens, increasing encoding efficiency and enhancing basic semantic understanding. By incorporating secondary pre-training using Chinese data and fine-tuning with Chinese instruction data, we substantially improve the models' comprehension and execution of instructions. Our pilot study serves as a foundation for researchers adapting LLaMA and Alpaca models to other languages. Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond. GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca",
        "strip_abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI).",
        "arxiv_url": "https://arxiv.org/pdf/2304.08177v1.pdf",
        "entity_stars": "8,293",
        "stars_accumulated": "0.66 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/ymcui/chinese-llama-alpaca",
            "https://github.com/jackaduma/Alpaca-LoRA-RLHF-PyTorch"
        ]
    },
    {
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "authors": "ICLR 2022, Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen",
        "gitlab": "microsoft/LoRA",
        "date": "Edward J. Hu",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/pgr-0001790295-7dc3f306_icyqJBh.jpg",
        "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
        "strip_abstract": "We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.",
        "arxiv_url": "https://arxiv.org/pdf/2106.09685v2.pdf",
        "entity_stars": "3,877",
        "stars_accumulated": "0.66 stars / hour",
        "paper_task": [
            "Language Modelling"
        ],
        "code": [
            "https://github.com/microsoft/LoRA",
            "https://github.com/tatsu-lab/stanford_alpaca",
            "https://github.com/tloen/alpaca-lora",
            "https://github.com/Lightning-AI/lit-llama",
            "https://github.com/videocrafter/videocrafter"
        ]
    },
    {
        "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
        "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi",
        "gitlab": "salesforce/lavis",
        "date": "11 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/paper/2305.06500.jpg",
        "abstract": "General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction. The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger Flamingo. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models have been open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.",
        "strip_abstract": "In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models.",
        "arxiv_url": "https://arxiv.org/pdf/2305.06500v1.pdf",
        "entity_stars": "4,410",
        "stars_accumulated": "0.63 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/salesforce/lavis"
        ]
    },
    {
        "title": "Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving",
        "authors": "Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, Hongyang Li",
        "gitlab": "opendrivelab/thinktwice",
        "date": "10 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/2264db83-aafc-4959-a9f0-0a73b2d9728b.jpg",
        "abstract": "End-to-end autonomous driving has made impressive progress in recent years. Existing methods usually adopt the decoupled encoder-decoder paradigm, where the encoder extracts hidden features from raw sensor data, and the decoder outputs the ego-vehicle's future trajectories or actions. Under such a paradigm, the encoder does not have access to the intended behavior of the ego agent, leaving the burden of finding out safety-critical regions from the massive receptive field and inferring about future situations to the decoder. Even worse, the decoder is usually composed of several simple multi-layer perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a combination of heavy ResNets or Transformer). Such an imbalanced resource-task division hampers the learning process. In this work, we aim to alleviate the aforementioned problem by two principles: (1) fully utilizing the capacity of the encoder; (2) increasing the capacity of the decoder. Concretely, we first predict a coarse-grained future position and action based on the encoder features. Then, conditioned on the position and action, the future scene is imagined to check the ramification if we drive accordingly. We also retrieve the encoder features around the predicted coordinate to obtain fine-grained information about the safety-critical region. Finally, based on the predicted future and the retrieved salient feature, we refine the coarse-grained position and action by predicting its offset from ground-truth. The above refinement module could be stacked in a cascaded fashion, which extends the capacity of the decoder with spatial-temporal prior knowledge about the conditioned future. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance in closed-loop benchmarks. Extensive ablation studies demonstrate the effectiveness of each proposed module.",
        "strip_abstract": "End-to-end autonomous driving has made impressive progress in recent years.",
        "arxiv_url": "https://arxiv.org/pdf/2305.06242v1.pdf",
        "entity_stars": "50",
        "stars_accumulated": "0.61 stars / hour",
        "paper_task": [
            "Autonomous Driving"
        ],
        "code": [
            "https://github.com/opendrivelab/thinktwice"
        ]
    },
    {
        "title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question",
        "authors": "Hong Wang, Xuan Luo, Weizhi Wang, Xifeng Yan",
        "gitlab": "hongwang600/flair",
        "date": "10 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/paper/2305.06424.jpg",
        "abstract": "Large language models like ChatGPT have recently demonstrated impressive capabilities in natural language understanding and generation, enabling various applications including translation, essay writing, and chit-chatting. However, there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks. Therefore, it is crucial to develop methods for detecting whether the party involved in a conversation is a bot or a human. In this paper, we propose a framework named FLAIR, Finding Large language model Authenticity via a single Inquiry and Response, to detect conversational bots in an online manner. Specifically, we target a single question scenario that can effectively differentiate human users from bots. The questions are divided into two categories: those that are easy for humans but difficult for bots (e.g., counting, substitution, positioning, noise filtering, and ASCII art), and those that are easy for bots but difficult for humans (e.g., memorization and computation). Our approach shows different strengths of these questions in their effectiveness, providing a new way for online service providers to protect themselves against nefarious activities and ensure that they are serving real users. We open-sourced our dataset on https://github.com/hongwang600/FLAIR and welcome contributions from the community to enrich such detection datasets.",
        "strip_abstract": "Large language models like ChatGPT have recently demonstrated impressive capabilities in natural language understanding and generation, enabling various applications including translation, essay writing, and chit-chatting.",
        "arxiv_url": "https://arxiv.org/pdf/2305.06424v1.pdf",
        "entity_stars": "26",
        "stars_accumulated": "0.60 stars / hour",
        "paper_task": [
            "Language Modelling",
            "Memorization",
            "Natural Language Understanding"
        ],
        "code": [
            "https://github.com/hongwang600/flair"
        ]
    },
    {
        "title": "SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation",
        "authors": "Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang",
        "gitlab": "winfredy/sadtalker",
        "date": "22 Nov 2022",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/fd3ca42e-002c-4af5-881c-940a0c1303d4.jpg",
        "abstract": "Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality.",
        "strip_abstract": "We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation.",
        "arxiv_url": "https://arxiv.org/pdf/2211.12194v2.pdf",
        "entity_stars": "3,367",
        "stars_accumulated": "0.58 stars / hour",
        "paper_task": [
            "Talking Head Generation"
        ],
        "code": [
            "https://github.com/winfredy/sadtalker"
        ]
    },
    {
        "title": "Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models",
        "authors": "Fangkai Jiao, Bosheng Ding, Tianze Luo, Zhanfeng Mo",
        "gitlab": "dandelionsllm/pandallm",
        "date": "4 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/cfc9ca45-a943-4aa1-833c-17649250a4d0.jpg",
        "abstract": "This project focuses on enhancing open-source large language models through instruction-tuning and providing comprehensive evaluations of their performance. We explore how various training data factors, such as quantity, quality, and linguistic distribution, influence the performance of instruction-tuned models trained on publicly accessible high-quality instruction datasets for both English and Chinese languages. Our goal is to supplement evaluation with quantitative analyses, providing valuable insights for the continued advancement of open-source chat models. Our model, data, and code are publicly available for others to use and build upon.",
        "strip_abstract": "This project focuses on enhancing open-source large language models through instruction-tuning and providing comprehensive evaluations of their performance.",
        "arxiv_url": "https://arxiv.org/pdf/2305.03025v1.pdf",
        "entity_stars": "588",
        "stars_accumulated": "0.57 stars / hour",
        "paper_task": [
            "Instruction Following"
        ],
        "code": [
            "https://github.com/dandelionsllm/pandallm"
        ]
    },
    {
        "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
        "authors": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",
        "gitlab": "nlpxucan/wizardlm",
        "date": "24 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/7145d7a2-6881-486f-b082-0626a35a9e43.jpg",
        "abstract": "Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models. Our codes and generated data are public at https://github.com/nlpxucan/WizardLM",
        "strip_abstract": "By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT.",
        "arxiv_url": "https://arxiv.org/pdf/2304.12244v1.pdf",
        "entity_stars": "1,566",
        "stars_accumulated": "0.55 stars / hour",
        "paper_task": [
            "Instruction Following"
        ],
        "code": [
            "https://github.com/nlpxucan/wizardlm"
        ]
    },
    {
        "title": "Pretraining Without Attention",
        "authors": "Junxiong Wang, Jing Nathan Yan, Albert Gu, Alexander M. Rush",
        "gitlab": "jxiw/bigs",
        "date": "20 Dec 2022",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/4a923fd9-6777-490f-984e-18a8f6bccc38.jpg",
        "abstract": "Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://github.com/jxiw/BiGS.",
        "strip_abstract": "Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation.",
        "arxiv_url": "https://arxiv.org/pdf/2212.10544v2.pdf",
        "entity_stars": "54",
        "stars_accumulated": "0.55 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/jxiw/bigs"
        ]
    },
    {
        "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
        "authors": "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao",
        "gitlab": "Lightning-AI/lit-llama",
        "date": "28 Mar 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/c278a6af-eb5e-471f-880d-45a5684d50f0.jpg",
        "abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapter.",
        "strip_abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model.",
        "arxiv_url": "https://arxiv.org/pdf/2303.16199v1.pdf",
        "entity_stars": "3,689",
        "stars_accumulated": "0.54 stars / hour",
        "paper_task": [
            "Instruction Following",
            "Language Modelling",
            "Multimodal Deep Learning",
            "Visual Question Answering (VQA)"
        ],
        "code": [
            "https://github.com/zrrskywalker/llama-adapter",
            "https://github.com/Lightning-AI/lit-llama"
        ]
    },
    {
        "title": "Towards Building the Federated GPT: Federated Instruction Tuning",
        "authors": "Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin Wang, Yiran Chen",
        "gitlab": "jayzhang42/federatedgpt-shepherd",
        "date": "9 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/a6c5aceb-c3c4-4c89-9a82-abd0e6409d7e.jpg",
        "abstract": "While ``instruction-tuned\" generative large language models (LLMs) have demonstrated an impressive ability to generalize to new tasks, the training phases heavily rely on large amounts of diverse and high-quality instruction data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. Consequently, this hinders the generality of the tuned models and may restrict their effectiveness in certain contexts. To tackle this issue, our study introduces a new approach called Federated Instruction Tuning (FedIT), which leverages federated learning (FL) as the learning framework for the instruction tuning of LLMs. This marks the first exploration of FL-based instruction tuning for LLMs. This is especially important since text data is predominantly generated by end users. Therefore, it is imperative to design and adapt FL approaches to effectively leverage these users' diverse instructions stored on local devices, while preserving privacy and ensuring data security. In the current paper, by conducting widely used GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous and diverse sets of instructions on the client's end with the proposed framework FedIT, we improved the performance of LLMs compared to centralized training with only limited local instructions. Further, in this paper, we developed a Github repository named Shepherd. This repository offers a foundational framework for exploring federated fine-tuning of LLMs using heterogeneous instructions across diverse categories.",
        "strip_abstract": "This repository offers a foundational framework for exploring federated fine-tuning of LLMs using heterogeneous instructions across diverse categories.",
        "arxiv_url": "https://arxiv.org/pdf/2305.05644v1.pdf",
        "entity_stars": "58",
        "stars_accumulated": "0.52 stars / hour",
        "paper_task": [
            "Federated Learning"
        ],
        "code": [
            "https://github.com/jayzhang42/federatedgpt-shepherd"
        ]
    },
    {
        "title": "Track Anything: Segment Anything Meets Videos",
        "authors": "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng",
        "gitlab": "gaomingqi/track-anything",
        "date": "24 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/a271a052-2b62-45de-aeab-25dceba63569.jpg",
        "abstract": "Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with very little human participation, i.e., several clicks, people can track anything they are interested in, and get satisfactory results in one-pass inference. Without additional training, such an interactive design performs impressively on video object tracking and segmentation. All resources are available on {https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitate related research.",
        "strip_abstract": "Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos.",
        "arxiv_url": "https://arxiv.org/pdf/2304.11968v2.pdf",
        "entity_stars": "4,318",
        "stars_accumulated": "0.50 stars / hour",
        "paper_task": [
            "Image Segmentation",
            "Object Tracking",
            "Semantic Segmentation",
            "Video Object Tracking"
        ],
        "code": [
            "https://github.com/gaomingqi/track-anything"
        ]
    },
    {
        "title": "ZipIt! Merging Models from Different Tasks without Training",
        "authors": "George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, Judy Hoffman",
        "gitlab": "gstoica27/zipit",
        "date": "4 May 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/82b2d85f-dac9-440a-ac5b-b0222b9bc1fe.jpg",
        "abstract": "Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce \"ZipIt!\", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general \"zip\" operation. Second, we add support for partially zipping the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for a staggering 20-60% improvement over prior work, making the merging of models trained on disjoint tasks feasible.",
        "strip_abstract": "While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks.",
        "arxiv_url": "https://arxiv.org/pdf/2305.03053v1.pdf",
        "entity_stars": "127",
        "stars_accumulated": "0.47 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/gstoica27/zipit"
        ]
    },
    {
        "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
        "authors": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang",
        "gitlab": "x-plug/mplug-owl",
        "date": "27 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/e133fb44-146d-4ef1-9ebe-1b0eb9653cf8.gif",
        "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.",
        "strip_abstract": "Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github. com/X-PLUG/mPLUG-Owl.",
        "arxiv_url": "https://arxiv.org/pdf/2304.14178v1.pdf",
        "entity_stars": "719",
        "stars_accumulated": "0.43 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/x-plug/mplug-owl"
        ]
    },
    {
        "title": "Enhancing Suno's Bark Text-to-Speech Model: Addressing Limitations Through Meta's Encodec and Pre-Trained Hubert",
        "authors": "Social Science Research Network (SSRN) 2023, Devin Schumacher, Francis LaBounty Jr.",
        "gitlab": "serp-ai/bark-with-voice-clone",
        "date": "Devin Schumacher",
        "cover_img": "data:image/jpeg;base64,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",
        "abstract": "Bark, a transformer-based text-to-audio model by Suno, generates highly realistic, multilingual speech as well as other audio, including music, background noise, and simple sound effects. While this model has shown promising results, its generative nature can lead to deviations in the output based on provided prompts. In this paper, we propose a novel approach to improve the performance of Bark by leveraging Meta's encodec to extract audio codebooks and employing a pre-trained HuBert model with a linear projection head to generate semantic tokens that better match the source audio.\r\n\r\nOur method involves extracting discrete tokens from the audio codebooks using Meta's encodec and saving the fine and coarse prompts. We then use the transcript of the source audio to generate semantic tokens from the original Bark model. However, this process has limitations due to the lack of access to the wav2vec model and its associated k-means used in the original training. To overcome this, we adapt a pre-trained HuBert model with a linear projection head, training it to output tokens in the same embedding space as the unavailable model.\r\n\r\nBy incorporating these enhancements, we aim to address the limitations of Bark's generative capabilities, ultimately leading to more accurate text-to-speech outputs. Our work contributes to the ongoing development of advanced artificial intelligence for text-to-audio generation and supports the research community by providing access to pre-trained model checkpoints, which are ready for inference and available for commercial use.\r\n\r\nKeywords: Bark, ai voice cloning, Suno, text-to-speech, artificial intelligence, audio generation, Meta's encodec, audio codebooks, semantic tokens, HuBert, transformer-based model, multilingual speech, wav2vec, linear projection head, embedding space, generative capabilities, pretrained model checkpoints",
        "strip_abstract": "Keywords: Bark, ai voice cloning, Suno, text-to-speech, artificial intelligence, audio generation, Meta's encodec, audio codebooks, semantic tokens, HuBert, transformer-based model, multilingual speech, wav2vec, linear projection head, embedding space, generative capabilities, pretrained model checkpoints",
        "arxiv_url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4443815",
        "entity_stars": "1,206",
        "stars_accumulated": "0.42 stars / hour",
        "paper_task": [
            "Audio Generation",
            "Expressive Speech Synthesis",
            "Speech Synthesis",
            "Text-To-Speech Synthesis",
            "Voice Cloning"
        ],
        "code": [
            "https://github.com/serp-ai/bark-with-voice-clone",
            "https://hub.serp.ai/serpdotai/bark-suno-enhancement-text-to-speech-ai-voice-cloning-app"
        ]
    },
    {
        "title": "Magic3D: High-Resolution Text-to-3D Content Creation",
        "authors": "Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin",
        "gitlab": "chinhsuanwu/dreamfusionacc",
        "date": "18 Nov 2022",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/paper/2211.10440.jpg",
        "abstract": "DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.",
        "strip_abstract": "DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results.",
        "arxiv_url": "https://arxiv.org/pdf/2211.10440v2.pdf",
        "entity_stars": "40",
        "stars_accumulated": "0.42 stars / hour",
        "paper_task": [
            "High",
            "Text to 3D"
        ],
        "code": [
            "https://github.com/chinhsuanwu/dreamfusionacc"
        ]
    },
    {
        "title": "Universal Instance Perception as Object Discovery and Retrieval",
        "authors": "Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, Huchuan Lu",
        "gitlab": "MasterBin-IIAU/UNINEXT",
        "date": "12 Mar 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/35e486b3-bc85-4c5e-9682-27823969beb5.jpg",
        "abstract": "All instance perception tasks aim at finding certain objects specified by some queries such as category names, language expressions, and target annotations, but this complete field has been split into multiple independent subtasks. In this work, we present a universal instance perception model of the next generation, termed UNINEXT. UNINEXT reformulates diverse instance perception tasks into a unified object discovery and retrieval paradigm and can flexibly perceive different types of objects by simply changing the input prompts. This unified formulation brings the following benefits: (1) enormous data from different tasks and label vocabularies can be exploited for jointly training general instance-level representations, which is especially beneficial for tasks lacking in training data. (2) the unified model is parameter-efficient and can save redundant computation when handling multiple tasks simultaneously. UNINEXT shows superior performance on 20 challenging benchmarks from 10 instance-level tasks including classical image-level tasks (object detection and instance segmentation), vision-and-language tasks (referring expression comprehension and segmentation), and six video-level object tracking tasks. Code is available at https://github.com/MasterBin-IIAU/UNINEXT.",
        "strip_abstract": "All instance perception tasks aim at finding certain objects specified by some queries such as category names, language expressions, and target annotations, but this complete field has been split into multiple independent subtasks.",
        "arxiv_url": "https://arxiv.org/pdf/2303.06674v1.pdf",
        "entity_stars": "1,099",
        "stars_accumulated": "0.41 stars / hour",
        "paper_task": [
            "Instance Segmentation",
            "Multi-Object Tracking and Segmentation",
            "Multiple Object Tracking",
            "object-detection",
            "Object Detection",
            "Object Discovery",
            "Object Tracking",
            "Referring Expression",
            "Referring Expression Comprehension",
            "Referring Expression Segmentation",
            "Referring Video Object Segmentation",
            "Retrieval",
            "Semantic Segmentation",
            "Video Instance Segmentation",
            "Visual Object Tracking",
            "Visual Tracking"
        ],
        "code": [
            "https://github.com/MasterBin-IIAU/UNINEXT"
        ]
    },
    {
        "title": "Measuring Massive Multitask Chinese Understanding",
        "authors": "Hui Zeng",
        "gitlab": "thudm/chatglm-6b",
        "date": "25 Apr 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/e0c13637-22da-4c1b-898c-176a96c06e8d.gif",
        "abstract": "The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 22 percentage points on average. Across the four major domains, the average zero-shot accuracy of all models did not exceed 0.5. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.703 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.259. By comprehensively evaluating the breadth and depth of knowledge across multiple disciplines, this test can more accurately identify the shortcomings of the models.",
        "strip_abstract": "This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education.",
        "arxiv_url": "https://arxiv.org/pdf/2304.12986v1.pdf",
        "entity_stars": "24,152",
        "stars_accumulated": "0.40 stars / hour",
        "paper_task": [],
        "code": [
            "https://github.com/Felixgithub2017/MMCU",
            "https://github.com/thudm/chatglm-6b"
        ]
    },
    {
        "title": "A Survey of Large Language Models",
        "authors": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, YiFan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen",
        "gitlab": "rucaibox/llmsurvey",
        "date": "31 Mar 2023",
        "cover_img": "https://production-media.paperswithcode.com/thumbnails/papergithubrepo/b734db0f-aac3-4825-b340-067ec4dca8f6.jpg",
        "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.",
        "strip_abstract": "To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size.",
        "arxiv_url": "https://arxiv.org/pdf/2303.18223v10.pdf",
        "entity_stars": "1,846",
        "stars_accumulated": "0.40 stars / hour",
        "paper_task": [
            "Language Modelling"
        ],
        "code": [
            "https://github.com/rucaibox/llmsurvey"
        ]
    }
]